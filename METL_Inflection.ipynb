{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabrice-camus/UNIGE-MA2a-METL-Projet-Inflection/blob/master/METL_Inflection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_5IYD3mFfQC"
      },
      "source": [
        "# METL // Projet - Morphological Inflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gjbIORxFa8Z"
      },
      "source": [
        "author = \"Fabrice Camus\"\n",
        "\n",
        "module = \"METL // SP 2021-2022\"\n",
        "\n",
        "version = \"1.0\"\n",
        "\n",
        "date = \"14.05.2022\"\n",
        "\n",
        "description = \"Given a lemma and POS, generate an inflected form, by using a seq2seq model\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Clone GitHub Repository"
      ],
      "metadata": {
        "id": "4HfPw_mTdEfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo = 'UNIGE-MA2a-METL-Projet-Inflection'"
      ],
      "metadata": {
        "id": "QGFELmKNdiO2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fabrice-camus/{repo}.git"
      ],
      "metadata": {
        "id": "V9pvl5PPc5tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6f7bfd-2e4d-406a-dc5d-a1c9de6bffa3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UNIGE-MA2a-METL-Projet-Inflection'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 84 (delta 32), reused 69 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (84/84), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {repo}"
      ],
      "metadata": {
        "id": "lDORjtg3dUq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9d878b-1eb1-4d4c-c01b-40692d1d7dd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UNIGE-MA2a-METL-Projet-Inflection/UNIGE-MA2a-METL-Projet-Inflection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzNbjckOBhmz"
      },
      "source": [
        "#2. Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "oIzxPkPehs8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SbYCZpitkR11"
      },
      "outputs": [],
      "source": [
        "# MorphoInflection_dataset\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from itertools import zip_longest\n",
        "\n",
        "\n",
        "START_CHAR ='<'\n",
        "END_CHAR ='>'\n",
        "EMPTY_CHAR =''\n",
        "\n",
        "#Pad two lists to have same length\n",
        "def pad(*lists, padding=0):\n",
        "    padded = [[] for _ in lists]\n",
        "\n",
        "    for lst in zip_longest(*lists, fillvalue=padding):\n",
        "        for i, elem in enumerate(lst):\n",
        "            padded[i].append(elem)\n",
        "\n",
        "    return padded\n",
        "\n",
        "#custom collate for batch creation (dataloader)\n",
        "def my_custom_collate(data):\n",
        "    # data is a list of tuples\n",
        "    x = [torch.tensor(d[0],dtype=torch.long) for d in data]\n",
        "    y = [torch.tensor(d[1],dtype=torch.long) for d in data]\n",
        "    x = pad_sequence(x, batch_first=True)\n",
        "    y = pad_sequence(y, batch_first=True)\n",
        "    l = [d[2] for d in data]\n",
        "    msd = [d[3] for d in data]\n",
        "    return x,y,l, msd\n",
        "\n",
        "\n",
        "class MorphoInflectionDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    super(MorphoInflectionDataset, self).__init__()\n",
        "    self.token2id={}\n",
        "    self.token2id[EMPTY_CHAR] = 0\n",
        "    self.token2id[START_CHAR] = 1\n",
        "    self.token2id[END_CHAR] = 2\n",
        "\n",
        "    self.id2token=[]\n",
        "    self.vocab_size=0\n",
        "\n",
        "\n",
        "  # load file and extract lines\n",
        "  def create_dataset(self,filename):\n",
        "    #file is encoded in utf-8 \n",
        "    with open(filename, encoding='utf_8') as f:\n",
        "      lines = f.readlines()\n",
        "      lines = [line.rstrip() for line in lines]\n",
        "     \n",
        "    #load raw data\n",
        "    self.corpus = lines\n",
        "    # create/update vocabulary and input/output in text format\n",
        "    self.input, self.output, self.lemmas, self.msd = self.__build_raw_data__()\n",
        "    #size of vocabulary\n",
        "    self.vocab_size = len(self.token2id)\n",
        "\n",
        "    # create a vector with token's id instead of text\n",
        "    self.input_id = [self.__tokens2id__(input) for input in self.input]\n",
        "    self.output_id = [self.__tokens2id__(output) for output in self.output]\n",
        "\n",
        "    # pad to have same length    \n",
        "    for i in range(len(self.input_id)):\n",
        "      self.input_id[i-1], self.output_id[i-1] = pad(self.input_id[i-1], self.output_id[i-1])\n",
        "\n",
        "\n",
        "  # create/update vocabulary, input and output raw data\n",
        "  def __build_raw_data__(self):\n",
        "  \n",
        "    #start index\n",
        "    index=len(self.token2id)\n",
        "    lemmas=[]\n",
        "    msds=[]\n",
        "    input = []\n",
        "    output = []\n",
        "    # extract token (char) and construct all we need\n",
        "    for line in self.corpus:\n",
        "      \n",
        "      datas=line.split('\\t')\n",
        "      \n",
        "      lemma=datas[0]\n",
        "      lemmas.append(lemma)\n",
        "      msd=datas[1]\n",
        "      msds.append(msd)\n",
        "      inflected_form=datas[2]\n",
        "      \n",
        "      lemma_chars=[]\n",
        "      \n",
        "      #split ääkköstää into char list\n",
        "      for c in lemma:\n",
        "        lemma_chars.append(c)\n",
        "        if c not in self.token2id:\n",
        "          self.token2id[c]=index\n",
        "          index+=1\n",
        "      \n",
        "      #idem for inflected form\n",
        "      inflected_form_chars = []\n",
        "      for c in inflected_form:\n",
        "        inflected_form_chars.append(c)\n",
        "        if c not in self.token2id:\n",
        "          self.token2id[c]=index\n",
        "          index+=1\n",
        "      \n",
        "\n",
        "      #extract POS-TAG : each pos-tag is a token \n",
        "      msd_tags=msd.split(',')\n",
        "      msd_chars_input=[]  \n",
        "      msd_chars_output=[]\n",
        "      for tag in msd_tags:\n",
        "        msd_chars_input.append(tag)\n",
        "        msd_chars_output.append(EMPTY_CHAR)\n",
        "        if tag not in self.token2id:\n",
        "          self.token2id[tag]=index\n",
        "          index+=1\n",
        "        #pad with empty char at beginning to have a alignement POS-TAG->''\n",
        "        #inflected_form_chars.insert(0,EMPTY_CHAR)\n",
        "        \n",
        "      #concat to construct final x and y in training data\n",
        "      #input_final=[*msd_tags,*lemma_chars]\n",
        "      input_final=[*lemma_chars,*msd_chars_input]\n",
        "      input_final.insert(0,START_CHAR)\n",
        "      input_final.append(END_CHAR)\n",
        "      inflected_form_chars_final=[*inflected_form_chars,*msd_chars_output]\n",
        "      inflected_form_chars_final.insert(0,START_CHAR)\n",
        "      inflected_form_chars_final.append(END_CHAR)\n",
        "\n",
        "      input.append(input_final)\n",
        "      output.append(inflected_form_chars_final)\n",
        "\n",
        "\n",
        "    # token list\n",
        "    self.id2token = list(self.token2id.keys())\n",
        "    \n",
        "    return input, output, lemmas, msds \n",
        "\n",
        "\n",
        "  def __tokens2id__(self,tokens):\n",
        "      return [self.token2id[token] for token in tokens]\n",
        "\n",
        "\n",
        "  # len of train_data\n",
        "  # must be implemented\n",
        "  def __len__(self):\n",
        "    return len(self.input_id) \n",
        "\n",
        "  # return x and y\n",
        "  # must be implemented\n",
        "  def __getitem__(self, idx):\n",
        "    x = self.input_id[idx]\n",
        "    y = self.output_id[idx]\n",
        "    lemma = self.lemmas[idx]\n",
        "    msd = self.msd[idx]\n",
        "    #tuple (x,y)\n",
        "    return x, y, lemma, msd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Sz82NjtfogHE"
      },
      "outputs": [],
      "source": [
        "train_dataset = MorphoInflectionDataset()\n",
        "train_dataset.create_dataset('./data/finnish-task1-train.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "C6IIOo01xP8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92037d9d-acec-4bfb-844a-04d5d61b19ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12693"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# data sample count\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RNDqRPB2q70_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296f2e8c-9b6c-405f-e6e0-46d37db6171a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 0,\n",
              " '-': 56,\n",
              " '<': 1,\n",
              " '>': 2,\n",
              " 'a': 20,\n",
              " 'aspect=PROSP': 19,\n",
              " 'b': 55,\n",
              " 'c': 61,\n",
              " 'case=ACC': 52,\n",
              " 'case=COM': 60,\n",
              " 'case=FRML': 27,\n",
              " 'case=GEN': 44,\n",
              " 'case=IN+ABL': 46,\n",
              " 'case=IN+ESS': 30,\n",
              " 'case=IN+LAT': 49,\n",
              " 'case=INS': 53,\n",
              " 'case=NOM': 38,\n",
              " 'case=ON+ABL': 39,\n",
              " 'case=ON+ALL': 50,\n",
              " 'case=ON+ESS': 24,\n",
              " 'case=PRIV': 37,\n",
              " 'case=PRT': 47,\n",
              " 'case=TRANS': 40,\n",
              " 'comp=CMPR': 69,\n",
              " 'comp=SPRL': 70,\n",
              " 'd': 36,\n",
              " 'e': 29,\n",
              " 'f': 59,\n",
              " 'finite=NFIN': 58,\n",
              " 'g': 54,\n",
              " 'h': 42,\n",
              " 'i': 16,\n",
              " 'j': 51,\n",
              " 'k': 4,\n",
              " 'l': 17,\n",
              " 'm': 15,\n",
              " 'mood=COND': 31,\n",
              " 'mood=IMP': 11,\n",
              " 'mood=IND': 43,\n",
              " 'mood=POT': 35,\n",
              " 'mood=PURP': 62,\n",
              " 'n': 8,\n",
              " 'num=PL': 25,\n",
              " 'num=SG': 14,\n",
              " 'o': 21,\n",
              " 'p': 28,\n",
              " 'per=1': 32,\n",
              " 'per=2': 34,\n",
              " 'per=3': 13,\n",
              " 'polar=POS': 10,\n",
              " 'pos=ADJ': 26,\n",
              " 'pos=N': 23,\n",
              " 'pos=V': 9,\n",
              " 'q': 71,\n",
              " 'r': 41,\n",
              " 's': 6,\n",
              " 't': 7,\n",
              " 'tense=PRS': 12,\n",
              " 'tense=PST': 33,\n",
              " 'u': 22,\n",
              " 'v': 45,\n",
              " 'voice=ACT': 18,\n",
              " 'voice=PASS': 57,\n",
              " 'w': 64,\n",
              " 'x': 67,\n",
              " 'y': 48,\n",
              " 'z': 63,\n",
              " 'â': 66,\n",
              " 'ä': 3,\n",
              " 'é': 65,\n",
              " 'ö': 5,\n",
              " 'ü': 68}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# vocabulary\n",
        "train_dataset.token2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MjHQlidhrc5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1d3e7b-1439-4fa7-ba08-c8648b1475d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '<',\n",
              " '>',\n",
              " 'ä',\n",
              " 'k',\n",
              " 'ö',\n",
              " 's',\n",
              " 't',\n",
              " 'n',\n",
              " 'pos=V',\n",
              " 'polar=POS',\n",
              " 'mood=IMP',\n",
              " 'tense=PRS',\n",
              " 'per=3',\n",
              " 'num=SG',\n",
              " 'm',\n",
              " 'i',\n",
              " 'l',\n",
              " 'voice=ACT',\n",
              " 'aspect=PROSP',\n",
              " 'a',\n",
              " 'o',\n",
              " 'u',\n",
              " 'pos=N',\n",
              " 'case=ON+ESS',\n",
              " 'num=PL',\n",
              " 'pos=ADJ',\n",
              " 'case=FRML',\n",
              " 'p',\n",
              " 'e',\n",
              " 'case=IN+ESS',\n",
              " 'mood=COND',\n",
              " 'per=1',\n",
              " 'tense=PST',\n",
              " 'per=2',\n",
              " 'mood=POT',\n",
              " 'd',\n",
              " 'case=PRIV',\n",
              " 'case=NOM',\n",
              " 'case=ON+ABL',\n",
              " 'case=TRANS',\n",
              " 'r',\n",
              " 'h',\n",
              " 'mood=IND',\n",
              " 'case=GEN',\n",
              " 'v',\n",
              " 'case=IN+ABL',\n",
              " 'case=PRT',\n",
              " 'y',\n",
              " 'case=IN+LAT',\n",
              " 'case=ON+ALL',\n",
              " 'j',\n",
              " 'case=ACC',\n",
              " 'case=INS',\n",
              " 'g',\n",
              " 'b',\n",
              " '-',\n",
              " 'voice=PASS',\n",
              " 'finite=NFIN',\n",
              " 'f',\n",
              " 'case=COM',\n",
              " 'c',\n",
              " 'mood=PURP',\n",
              " 'z',\n",
              " 'w',\n",
              " 'é',\n",
              " 'â',\n",
              " 'x',\n",
              " 'ü',\n",
              " 'comp=CMPR',\n",
              " 'comp=SPRL',\n",
              " 'q']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_dataset.id2token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ysewe97drdVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1059ddb-9fc7-451d-f156-0331256399f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# vocabulary size\n",
        "len(train_dataset.token2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8JJMatkdvx81"
      },
      "outputs": [],
      "source": [
        "# DataLoader is useful for batching\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=my_custom_collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xi8fJNlBl9u"
      },
      "source": [
        "#3. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oNg156bqOERg"
      },
      "outputs": [],
      "source": [
        "# MorphoInflection_model\n",
        "\n",
        "#help : https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "#help : https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350#2c97\n",
        "#help : https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
        "\n",
        "from torch.nn import Module\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import LSTM\n",
        "from torch.nn import Linear\n",
        "from torch.nn import Dropout\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "\n",
        "class morphological_inflection_seq2seq(Module):\n",
        "\n",
        "  def __init__(self, embedding_dim, hidden_size, vocab_size):\n",
        "    super(morphological_inflection_seq2seq, self).__init__()\n",
        "\n",
        "    # vocabulary size (int)\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # Embeddings dimensions (int)\n",
        "    self.embedding_size = embedding_dim\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell (int)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    #--------------\n",
        "    # Encoder\n",
        "    #--------------\n",
        "    p=0.5\n",
        "    self.input_dropout = Dropout(p)\n",
        "\n",
        "    # Embeddings : create a matrix of VxD\n",
        "    # shape(vocab_size,emb_dim)\n",
        "    # Embedding(72, 10)\n",
        "    self.inputs_embeds = Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_size, padding_idx=0)\n",
        "    \n",
        "\n",
        "    # LSTM(10, 256)\n",
        "    self.lstm_encoder = LSTM(self.embedding_size, self.hidden_size, batch_first=True, dropout=p)\n",
        "\n",
        "    #--------------\n",
        "    # Decoder\n",
        "    #--------------\n",
        "    \n",
        "    # 72\n",
        "    self.output_size = vocab_size\n",
        "    self.output_dropout = Dropout(p)\n",
        "    # Embeddings : create a matrix of VxD\n",
        "    # (vocab_size,emb_dim)\n",
        "    self.output_embeds = Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_size, padding_idx=0)\n",
        "    self.lstm_decoder = LSTM(self.embedding_size, self.hidden_size, batch_first=True, dropout=p)\n",
        "    #linear layer with input = hidden_size\n",
        "    #out_features = voc size -> so that we can apply a softmax activation function\n",
        "    #Linear(in_features=50, out_features=72, bias=True)\n",
        "    self.hidden2vocab = Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, batch, forceTeaching=False):\n",
        "      \n",
        "      # # x : list of n tensor (n=batch_size)\n",
        "      #x[i] : 1 x sample [ 1,  9, 10,  ...,  0,  0,  0]\n",
        "      x,y,_,_ = batch\n",
        "      \n",
        "      #batch_size x seq_len\n",
        "      y_matrix = np.array(y)\n",
        "\n",
        "      #--------------\n",
        "      # Encoder part\n",
        "      #--------------\n",
        "\n",
        "      # Emmbeddings\n",
        "      # torch.Size([batch_size, sequence_len, emb_dim])\n",
        "      # torch.Size([64, 26, 10])\n",
        "      embeds = self.input_dropout(self.inputs_embeds(x))\n",
        "\n",
        "      # pass embeds to lstm\n",
        "      # encoder_outputs : we don't care // torch.Size([batch_size, sequence_len, hidden_dim])\n",
        "      # encoder_hidden_state : last hiddend state which contains our input representation\n",
        "      # encoder_hidden_state: tuple(hidden state,cell state)\n",
        "      # hidden_state/cell_state : torch.Size([1, batch_size, hidden dim]) // torch.Size([1, 64, 50])\n",
        "      # 1 = unidirectionnel, 2 : bidir\n",
        "      encoder_outputs, encoder_hidden_state = self.lstm_encoder(embeds)\n",
        "\n",
        "\n",
        "      #--------------\n",
        "      # Decoder part\n",
        "      #--------------\n",
        "      # y : list of n tensors (n=batch_size)\n",
        "      #y[i] : 1 x sample [1, 0, 0,  ..., 2, 0, 0]\n",
        "      # ex : 26      \n",
        "      sequence_length = y_matrix.shape[1]\n",
        "      batch_size= y_matrix.shape[0]\n",
        "\n",
        "      # decoder lstm will receive hidden_state of encoder\n",
        "      hidden_state = encoder_hidden_state\n",
        "      # tensor for whole outputs --> contains id of predicted inflected form\n",
        "      # shape [seq_len, batch_size, self.vocab_size] //torch.Size([26, 64,72])\n",
        "      outputs = torch.zeros(sequence_length,batch_size,self.vocab_size)\n",
        "\n",
        "      #one char\n",
        "      #shape(batch_size,1)     \n",
        "      token=y[:,[0]]\n",
        "      \n",
        "      # for each char in y, call lstm to predict next char\n",
        "      # teach forcing --> give to next loop gold char (from y) and not predicted char\n",
        "      for i in range(1, sequence_length):\n",
        "\n",
        "        embeds = self.output_dropout(self.output_embeds(token) )\n",
        "        #LSTM layer\n",
        "        #output shape :[batch_size, seq_len, hidden size] torch.Size([64, 1, 50])\n",
        "        output, hidden_state = self.lstm_decoder(embeds, hidden_state)\n",
        "        \n",
        "        \n",
        "        #output : prediction of next token \n",
        "        #shape [batch_size,seq_len, vocab_size] // torch.Size([64, 1, 72])\n",
        "        prediction = self.hidden2vocab(output)\n",
        "\n",
        "        #shape [batch_size, vocab_size] // torch.Size([64, 72])\n",
        "        prediction = prediction.squeeze(1)\n",
        "\n",
        "        #torch.Size([64,72])\n",
        "        prediction_probs = softmax(prediction,1)\n",
        "       \n",
        "        outputs[i]=prediction_probs\n",
        "\n",
        "        # teacher forcing : next input in lstm isn't the predicted token, but the gold token (from y)\n",
        "        if forceTeaching:\n",
        "          token = y[:,[i]] \n",
        "        else:\n",
        "          #next token is the predicted token\n",
        "          token = torch.argmax(prediction_probs,1).view(prediction_probs.shape[0],1)\n",
        "          \n",
        "         \n",
        "\n",
        "\n",
        "      # -->seq_len, batch_size, self.vocab_size\n",
        "      outputs=torch.moveaxis(outputs,0,2)\n",
        "      return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Training"
      ],
      "metadata": {
        "id": "RMdOXAFueW1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-eUEsOsUuIQ"
      },
      "outputs": [],
      "source": [
        "##Training\n",
        "\n",
        "from torch.optim import SGD, Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Model\n",
        "emb_dim=10\n",
        "hidden_dim=256\n",
        "morpho_inflection_model = morphological_inflection_seq2seq(emb_dim, hidden_dim, train_dataset.vocab_size)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = SGD(morpho_inflection_model.parameters(), lr=0.1)\n",
        "\n",
        "# Loss\n",
        "cross_entropy_loss = CrossEntropyLoss()\n",
        "\n",
        "# Hyper-params\n",
        "num_epochs = 1000\n",
        "\n",
        "# Metrics\n",
        "losses_epoch_train = []\n",
        "\n",
        "\n",
        "\n",
        "# Loop\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  morpho_inflection_model.train()\n",
        "  epoch_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  batch_number=1\n",
        "  \n",
        "  for batch in train_loader:\n",
        "    print(f'Epoch ({epoch+1}/{num_epochs}) -  Batch ({batch_number}/{len(train_loader)}])')   \n",
        "\n",
        "    #batch --> tuples(x,y)\n",
        "    # \"shape\" : (batch_size,2)\n",
        "    # x : list of n tensor (n=batch_size)\n",
        "    #x[i] : 1 x sample [ 1,  9, 10,  ...,  0,  0,  0]\n",
        "    x, y,_,_ = batch\n",
        "\n",
        "    # call of forward function and get the output\n",
        "    # shape [batch_size,vocab_size,seq_len] //torch.Size([64, 72, 26])\n",
        "    y_scores = morpho_inflection_model(batch=batch,forceTeaching=True)\n",
        "\n",
        "    \n",
        "    # compute loss\n",
        "    # input (y_scores) must be [batch_size,number_of_classes,seq_len]\n",
        "    # target (y) must be [batch_size,seq_len]\n",
        "    loss = cross_entropy_loss(y_scores, y)\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  \n",
        "    #backpropagation   \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    batch_number+=1\n",
        "    \n",
        "    \n",
        "\n",
        "  losses_epoch_train.append(epoch_loss/len(train_loader))\n",
        "  \n",
        "  morpho_inflection_model.eval()\n",
        "  torch.save(morpho_inflection_model, './model/model.pth')\n",
        "  np.save('./model/losses_epoch_train.npy', losses_epoch_train)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0mz6natT_9PS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8d176fa4-09b7-4556-ee9b-fdfa181f9634"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "losses_epoch_train=np.load('./model/losses_epoch_train.npy')\n",
        "num_epochs=1000\n",
        "\n",
        "plt.plot(range(num_epochs),losses_epoch_train, label='train')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "#plt.savefig('./model/loss.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Evaluation"
      ],
      "metadata": {
        "id": "xeWPdCjiYaLw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NJTd3ddOr_xc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8c45ee-d94f-40fa-dfe2-0263722c039a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ],
      "source": [
        "from torch import argmax\n",
        "\n",
        "#train_dataset.create_dataset('./data/finnish-task1-train.txt')\n",
        "#train_dataset.create_dataset('./data/finnish-task1-dev')\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=my_custom_collate)\n",
        "\n",
        "emb_dim=10\n",
        "hidden_dim=256\n",
        "morpho_inflection_model = morphological_inflection_seq2seq(emb_dim, hidden_dim, train_dataset.vocab_size)\n",
        "\n",
        "morpho_inflection_model= torch.load('./model/model.pth')\n",
        "morpho_inflection_model.eval()\n",
        "\n",
        "#f = open('./data/finnish-task1-dev-guess', 'w')\n",
        "f = open('./data/finnish-task1-train-guess', 'w')\n",
        "    \n",
        "\n",
        "for batch in train_loader:\n",
        "  x, y,l,m = batch\n",
        "\n",
        "  #torch.Size([64, 72, 26])\n",
        "  y_scores = morpho_inflection_model(batch=batch,forceTeaching=False)\n",
        "    \n",
        "  prediction_string=''\n",
        "  for sample in range(len(x)):\n",
        "    #y_scores for one sample\n",
        "    y_sample=y_scores[sample]\n",
        "\n",
        "    prediction_id=torch.argmax(y_sample,0)\n",
        "    #print(prediction_id)\n",
        "    prediction_array=[train_dataset.id2token[p] for p in prediction_id]\n",
        "    prediction_string=''.join(prediction_array)\n",
        "    if prediction_string=='':\n",
        "      prediction_string='NULL'\n",
        "    f.write(l[sample]+'\\t'+m[sample]+'\\t'+prediction_string+'\\n')\n",
        "\n",
        "  \n",
        "f.close()  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2.7 evalm.py --golden=./data/finnish-task1-train.txt --guesses=./data/finnish-task1-train-guess"
      ],
      "metadata": {
        "id": "6xpqVJ26ZAbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da49f2c1-6eb7-4151-acb3-f91fcc53f127"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\n",
            "Accuracy: 0.0\n",
            "Mean Levenshtein: 10.3535002008\n",
            "Mean Normalized Levenshtein: 0.804215442593\n",
            "Mean Reciprocal Rank: 0.0\n",
            "\n",
            "ADJ\n",
            "Accuracy: 0.0\n",
            "Mean Levenshtein: 9.57352941176\n",
            "Mean Normalized Levenshtein: 0.81156526334\n",
            "Mean Reciprocal Rank: 0.0\n",
            "\n",
            "V\n",
            "Accuracy: 0.0\n",
            "Mean Levenshtein: 9.29252525253\n",
            "Mean Normalized Levenshtein: 0.815253453371\n",
            "Mean Reciprocal Rank: 0.0\n",
            "\n",
            "Aggregate\n",
            "Accuracy: 0.0\n",
            "Mean Levenshtein: 9.92302844087\n",
            "Mean Normalized Levenshtein: 0.808677532295\n",
            "Mean Reciprocal Rank: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2.7 evalm.py --golden=./data/finnish-task1-dev --guesses=./data/finnish-task1-dev-guess"
      ],
      "metadata": {
        "id": "z8W8MAixIstY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ceeda5-3c80-48b2-9a98-1b22295eb98c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\n",
            "Accuracy: 0.0\n",
            "Mean Levenshtein: 10.5088819227\n",
            "Mean Normalized Levenshtein: 0.801755787863\n",
            "Mean Reciprocal Rank: 0.0\n",
            "\n",
            "ADJ\n",
            "Accuracy: 0.0\n",
            "Mean Levenshtein: 8.83333333333\n",
            "Mean Normalized Levenshtein: 0.754135524969\n",
            "Mean Reciprocal Rank: 0.0\n",
            "\n",
            "V\n",
            "Accuracy: 0.0\n",
            "Mean Levenshtein: 9.42314049587\n",
            "Mean Normalized Levenshtein: 0.838341760564\n",
            "Mean Reciprocal Rank: 0.0\n",
            "\n",
            "Aggregate\n",
            "Accuracy: 0.0\n",
            "Mean Levenshtein: 10.0600750939\n",
            "Mean Normalized Levenshtein: 0.814534376111\n",
            "Mean Reciprocal Rank: 0.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "METL_Inflection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlK2kRNAJw3vO+ffAg5Cqc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}